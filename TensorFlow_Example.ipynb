{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECADES TensorFlow Examples\n",
    "\n",
    "Here we will walk through a simple Keras examples (from [the official Keras examples](https://github.com/keras-team/keras/tree/master/examples)) in its entirety for the TensorFlow flow through DECADES.\n",
    "\n",
    "We will go through a convolutional neural net example. Before reading this guide, we hope that you have familiarity with CNNs and have gone over:\n",
    "\n",
    "1. [Intro to DECADES programming document](intro.ipynb)\n",
    "2. [Intro to programming DECADES through TensorFlow](DECADES_TensorFlow.ipynb)\n",
    "\n",
    "## Location\n",
    "\n",
    "This example can be found in the docker at /decades-sdh/applications/tensorflow/examples/.\n",
    "\n",
    "## Convolutional Neural Network\n",
    "\n",
    "The example code is written in dec_mnist_cnn.py. We first import the necessary TensorFlow libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Tensorflow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to import the DECADES TensorFlow library, \"DEC_TensorFlow\" so that we can feed the computation graph to the DECADES framework and employ our tools to perform an analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DEC_TensorFlow as dtf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a function that makes it easier to dump and keep track of program traces. The function takes in a graph of all operations in the TensorFlow program (which is simply `tf.get_default_graph().get_operations()`) as well as the name of the file to where the DECADES TensorFlow C++ code is to be written. Within the function we employ the DECADES framework with `dtf.run()`. We also print out the important TensorFlow functions with `dtf.print_trace()`. This `dump_trace()` function is optional; you can simply pass in a C++ filename and `tf.get_default_graph()` to dtf.run()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_trace(ops, filename):\n",
    "\n",
    "    filedir = \"output/trace/\"\n",
    "\n",
    "    call_trace = open(filedir + filename + \".txt\", \"w\")\n",
    "\n",
    "    for op in ops:\n",
    "        call_trace.write(str(op.name) + \"\\n\")\n",
    "\n",
    "    call_trace.close()\n",
    "\n",
    "    args_trace_name = filedir + filename + \"_args.txt\"\n",
    "    with open(args_trace_name, 'w') as args_trace:\n",
    "        sys.stdout = args_trace\n",
    "        dtf.print_trace(tf.get_default_graph())\n",
    "        dtf.run(filedir + filename + \".cpp\", tf.get_default_graph())\n",
    "        sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write out our neural net training code. We first set our training parameters (batch size, number of classes, number of epochs, etc.) and load in data from the MNIST dataset. We also need to perform some data reshaping based on how the image data is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "# DECADES\n",
    "epochs = 1 # 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now utilize the Keras API functions to build our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile the model, perform training with `model.fit()`, and perform inference with `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can print out our accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we utilize our `dump_trace()` function by passing in 1) the operations graph, `tf.get_default_graph().get_operations()` and 2) the filename that we obtain from our current directory `os.path.splitext(os.path.basename(__file__))[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DECADES: dump TF function trace\n",
    "dump_trace(tf.get_default_graph().get_operations(), os.path.splitext(os.path.basename(__file__))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have generated a C++ file, `dec_mnist_cnn.cpp`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
