{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECADES TensorFlow Examples\n",
    "\n",
    "Here we will walk through a simple Keras examples (from [the official Keras examples](https://github.com/keras-team/keras/tree/master/examples)) in its entirety for the TensorFlow flow through DECADES.\n",
    "\n",
    "We will go through a convolutional neural net example. Before reading this guide, we hope that you have familiarity with CNNs and have gone over:\n",
    "\n",
    "1. [Intro to DECADES programming document](Introduction.ipynb)\n",
    "2. [Intro to programming DECADES through TensorFlow](Decades_Tensorflow_Pipeline.ipynb)\n",
    "\n",
    "## Location\n",
    "\n",
    "This example can be found in the docker at /decades/applications/tensorflow/examples/keras/.\n",
    "\n",
    "## Convolutional Neural Network\n",
    "\n",
    "The example code is written in dec_mnist_cnn.py. We first import the necessary TensorFlow libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Helper libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Tensorflow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to import the DECADES TensorFlow library, \"DEC_TensorFlow\" so that we can feed the computation graph to the DECADES framework and employ our tools to perform an analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECADES libraries\n",
    "import DEC_TensorFlow as dtf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write out our neural net training code. We first set our training parameters (batch size, number of classes, number of epochs, etc.) and load in data from the MNIST dataset. We also need to perform some data reshaping based on how the image data is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1 #12 originally\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now utilize the Keras API functions to build our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compile the model, perform training with `model.fit()`, and perform inference with `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can print out our accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we utilize our `dump_trace()` function by passing in 1) the filename that we obtain from our current directory `os.path.splitext(os.path.basename(__file__))[0]` and 2) the operations graph `tf.get_default_graph().get_operations()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DECADES: dump TF function trace\n",
    "dump_trace(os.path.splitext(os.path.basename(__file__))[0],\n",
    "           tf.get_default_graph().get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have finished writing the code for this example, you can run the application to test its correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate tf\n",
    "python dec_mnist_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the application has finished running, you should have generated a C++ file, `dec_mnist_cnn.cpp`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
